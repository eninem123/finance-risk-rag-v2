# bert_finetune.py å¤´éƒ¨å¯¼å…¥
from utils import clean_text, setup_logger, ensure_dirs
import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from torch.utils.data import Dataset, DataLoader
from transformers import (
    BertTokenizerFast,
    BertForTokenClassification,
    get_linear_schedule_with_warmup
)
from tqdm import tqdm
import os
# åˆå§‹åŒ–æ—¥å¿—ï¼ˆæ–¹ä¾¿æ’æŸ¥è®­ç»ƒé”™è¯¯ï¼‰
logger = setup_logger(name="bert_finetune", log_file="bert_train.log", level=logging.INFO)


# ====================== 1. å…¨å±€é…ç½®ï¼ˆå·²æŒ‰ä½ çš„è·¯å¾„ä¿®æ”¹ï¼‰======================
class Config:
    # è®­ç»ƒé›†å’ŒéªŒè¯é›†è·¯å¾„ï¼ˆä½ çš„å®é™…è·¯å¾„ï¼‰
    train_path = "dataset/train/ner_train.txt"  # ä½ çš„è®­ç»ƒé›†è·¯å¾„
    dev_path = "dataset/dev/ner_dev.txt"        # ä½ çš„éªŒè¯é›†è·¯å¾„
    # æ ‡ç­¾ä½“ç³»ï¼ˆä¸ä¹‹å‰çš„æ ‡æ³¨æ•°æ®ä¸€è‡´ï¼‰
    label_list = [
        "O", "B-DATE", "I-DATE", "B-PER", "I-PER", 
        "B-ORG", "I-ORG", "B-MONEY", "I-MONEY", 
        "B-RISK", "I-RISK", "B-SEC", "I-SEC", 
        "B-REG", "I-REG", "B-LAW", "I-LAW"
    ]
    label2id = {label: i for i, label in enumerate(label_list)}
    id2label = {i: label for i, label in enumerate(label_list)}
    # æ¨¡å‹ä¸è®­ç»ƒé…ç½®  åŠ è½½ä¸­æ–‡ BERT åˆ†ç±»æ¨¡å‹
    model_name = "hfl/chinese-bert-wwm-ext"
    max_seq_len = 512
    batch_size = 4  # CPUç¯å¢ƒå»ºè®®4ï¼Œé¿å…å†…å­˜ä¸è¶³
    epochs = 5
    learning_rate = 2e-5 # å¾®è°ƒå­¦ä¹ ç‡ï¼ˆç•¥ä½äºåŸ3e-5ï¼Œæ›´ç¨³å®šï¼‰
    save_dir = "bert_ner_model"  # ä½ çš„æ¨¡å‹ä¿å­˜è·¯å¾„
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

config = Config()

# ====================== 2. æ•°æ®é›†åŠ è½½ï¼ˆé€‚é…ä½ çš„è·¯å¾„ï¼‰======================
class NERDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_seq_len):
        self.data = self.load_conll_data(data_path)
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.label2id = config.label2id

    def load_conll_data(self, data_path):
        """åŠ è½½ä½ çš„CoNLLæ ¼å¼æ•°æ®ï¼ˆæŒ‰ä½ çš„è·¯å¾„è¯»å–ï¼‰"""
        data = []
        current_tokens = []
        current_labels = []
        
        try:
            with open(data_path, "r", encoding="utf-8") as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        if current_tokens:
                            data.append({"tokens": current_tokens, "labels": current_labels})
                            current_tokens = []
                            current_labels = []
                        continue
                    parts = line.split()
                    if len(parts) != 2:
                        logger.warning(f"ç¬¬{line_num}è¡Œæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡ â†’ {line}")  # ç”¨æ—¥å¿—æ›¿ä»£print
                        print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡Œæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡ â†’ {line}")
                        continue
                    token, label = parts[0], parts[1]
                    # æ–°å¢ï¼šæ¸…æ´—tokenï¼ˆé¿å…ç‰¹æ®Šå­—ç¬¦å½±å“è®­ç»ƒï¼‰
                    token = clean_text(token)
                    if token:  # è¿‡æ»¤ç©ºtoken
                    
                        current_tokens.append(token)
                        current_labels.append(label)
            
            if current_tokens:
                data.append({"tokens": current_tokens, "labels": current_labels})
            
            print(f"æˆåŠŸåŠ è½½{len(data)}æ¡å¥å­ï¼ˆæ¥è‡ª{data_path}ï¼‰")
            return data
        except FileNotFoundError:
            print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ {data_path}ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼")
            exit(1)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        tokens = item["tokens"]
        labels = item["labels"]
        
        encoding = self.tokenizer(
            tokens,
            is_split_into_words=True,
            max_length=config.max_seq_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        word_ids = encoding.word_ids(batch_index=0)
        aligned_labels = []
        
        for word_id in word_ids:
            if word_id is None:
                aligned_labels.append(-100)
            else:
                label = labels[word_id] if word_id < len(labels) else "O"
                aligned_labels.append(self.label2id[label])
        
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(aligned_labels, dtype=torch.long)
        }

# ====================== 3. æ¨¡å‹ä¸è®­ç»ƒå‡½æ•° =======================
def init_model_tokenizer():
    tokenizer = BertTokenizerFast.from_pretrained(config.model_name)
    model = BertForTokenClassification.from_pretrained(
        config.model_name,
        num_labels=len(config.label_list),
        id2label=config.id2label,
        label2id=config.label2id
    )
    model.to(config.device)
    return model, tokenizer

def train_epoch(model, dataloader, optimizer, scheduler):
    model.train()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    for batch in tqdm(dataloader, desc="Training"):
        input_ids = batch["input_ids"].to(config.device)
        attention_mask = batch["attention_mask"].to(config.device)
        labels = batch["labels"].to(config.device)
        
        optimizer.zero_grad()
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        loss = outputs.loss
        logits = outputs.logits
        
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item() * input_ids.size(0)
        preds = torch.argmax(logits, dim=2).cpu().numpy()
        labels_np = labels.cpu().numpy()
        
        for p, l in zip(preds, labels_np):
            mask = l != -100
            all_preds.extend(p[mask])
            all_labels.extend(l[mask])
    
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average="macro", zero_division=0
    )
    return avg_loss, accuracy, precision, recall, f1

def eval_epoch(model, dataloader):
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch["input_ids"].to(config.device)
            attention_mask = batch["attention_mask"].to(config.device)
            labels = batch["labels"].to(config.device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs.loss
            logits = outputs.logits
            
            total_loss += loss.item() * input_ids.size(0)
            preds = torch.argmax(logits, dim=2).cpu().numpy()
            labels_np = labels.cpu().numpy()
            
            for p, l in zip(preds, labels_np):
                mask = l != -100
                all_preds.extend(p[mask])
                all_labels.extend(l[mask])
    
    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average="macro", zero_division=0
    )
    return avg_loss, accuracy, precision, recall, f1

# ====================== 4. ä¸»å‡½æ•°ï¼ˆæŒ‰ä½ çš„è·¯å¾„ä¿å­˜æ¨¡å‹ï¼‰======================
def main():
    model, tokenizer = init_model_tokenizer()
    logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡ï¼š{config.device}")  # æ—¥å¿—è®°å½•
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡ï¼š{config.device}")
    # æ–°å¢ï¼šç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
    ensure_dirs(config.save_dir)
    # åŠ è½½ä½ çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_dataset = NERDataset(config.train_path, tokenizer, config.max_seq_len)
    dev_dataset = NERDataset(config.dev_path, tokenizer, config.max_seq_len)
    
    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False)
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼šè®­ç»ƒé›†{len(train_dataset)}æ¡ï¼ŒéªŒè¯é›†{len(dev_dataset)}æ¡")
    
    # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨PyTorchåŸç”Ÿå®ç°ï¼Œé¿å…è­¦å‘Šï¼‰
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, eps=1e-8)
    total_steps = len(train_loader) * config.epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=0, num_training_steps=total_steps
    )
    
    best_f1 = 0.0
    # åˆ›å»ºä½ çš„æ¨¡å‹ä¿å­˜ç›®å½•
    os.makedirs(config.save_dir, exist_ok=True)
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼ˆå…±{config.epochs}è½®ï¼‰ï¼š")
    
    for epoch in range(config.epochs):
        print(f"\n===== Epoch {epoch+1}/{config.epochs} =====")
        train_loss, train_acc, train_p, train_r, train_f1 = train_epoch(
            model, train_loader, optimizer, scheduler
        )
        dev_loss, dev_acc, dev_p, dev_r, dev_f1 = eval_epoch(model, dev_loader)
        
        print(f"è®­ç»ƒé›†ï¼šæŸå¤±={train_loss:.4f} | å‡†ç¡®ç‡={train_acc:.4f} | F1={train_f1:.4f}")
        print(f"éªŒè¯é›†ï¼šæŸå¤±={dev_loss:.4f} | å‡†ç¡®ç‡={dev_acc:.4f} | F1={dev_f1:.4f}")
        
        # ä¿å­˜åˆ°ä½ çš„è·¯å¾„
        if dev_f1 > best_f1:
            best_f1 = dev_f1
            model.save_pretrained(os.path.join(config.save_dir, "best_model"))
            tokenizer.save_pretrained(os.path.join(config.save_dir, "best_model"))
            print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{os.path.join(config.save_dir, 'best_model')}")
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯é›†F1ï¼š{best_f1:.4f}")
    print(f"æ¨¡å‹æœ€ç»ˆä¿å­˜è·¯å¾„ï¼š{config.save_dir}/best_model")

if __name__ == "__main__":
    main()