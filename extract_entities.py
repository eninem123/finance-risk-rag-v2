# extract_entities.py
# é“¶è¡Œçº§é£æ§ RAG ç³»ç»Ÿ - Kimi å¤§æ¨¡å‹ç‰ˆï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰
# åŠŸèƒ½ï¼šè§„åˆ™+BER TåŒæ¨¡å¼å®ä½“æå– + é£é™©è¯„åˆ† + Chromaå‘é‡åº“ + å¾ªç¯é—®ç­”
import json
import re
import os
import time
import shutil
from datetime import datetime
from typing import List, Dict
from tqdm import tqdm  # è¿›åº¦æ¡
from openai import OpenAI
import chromadb
from chromadb.utils.embedding_functions import ONNXMiniLM_L6_V2

# å¯¼å…¥é¡¹ç›®é€šç”¨å·¥å…·ï¼ˆå·²ä¼˜åŒ–çš„æ–‡æœ¬å¤„ç†ã€è·¯å¾„ç®¡ç†ç­‰ï¼‰
from utils import (
    clean_text, load_json_file, save_json_file,
    ensure_dirs, normalize_path, calculate_risk_level
)

# ====================== å…¨å±€é…ç½®ï¼ˆé€‚é…ä½ çš„é¡¹ç›®ç»“æ„ï¼‰=======================
# 1. è·¯å¾„é…ç½®ï¼ˆåŸºäºé¡¹ç›®æ ¹ç›®å½•ï¼Œé¿å…ç¡¬ç¼–ç ï¼‰
PROJECT_ROOT = normalize_path("")  # é¡¹ç›®æ ¹ç›®å½•
DOCS_DIR = normalize_path("docs")  # æ–‡æ¡£ç›®å½•ï¼ˆå­˜æ”¾å¾…å¤„ç†txtï¼‰
RULES_PATH = normalize_path("knowledge_base/risk_entities.json")  # å®ä½“è§„åˆ™åº“
RAG_DB_DIR = normalize_path("rag_db")  # Chromaå‘é‡åº“ç›®å½•
BERT_MODEL_PATH = normalize_path("bert_ner_model/best_model")  # è®­ç»ƒå¥½çš„BERTæ¨¡å‹è·¯å¾„
OUTPUT_JSON = normalize_path("docs/entities_extracted.json")  # å®ä½“ç»“æœä¿å­˜è·¯å¾„

# 2. Kimiå¤§æ¨¡å‹é…ç½®ï¼ˆæ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥ï¼‰
KIMI_API_KEY = "sk-VNPvMcWdMNXObfyi9fLMNSRBOYsmTgN420ugLlmV9z5RqxyE"  # ä½ çš„Kimi API Key
KIMI_BASE_URL = "https://api.moonshot.cn/v1"  # Kimiå›ºå®šæ¥å£åœ°å€
KIMI_MODEL = "moonshot-v1-8k"  # Kimiæ”¯æŒçš„æ¨¡å‹ï¼ˆé¿å…404é”™è¯¯ï¼‰

# 3. å®ä½“æå–é…ç½®
NUM_PATTERNS = {  # é‡‘èæ•°å­—å®ä½“æ­£åˆ™ï¼ˆè¦†ç›–é£æ§å…³é”®æŒ‡æ ‡ï¼‰
    "liquidity_risk": r'(ç°é‡‘å‚¨å¤‡|ç°é‡‘åŠç°é‡‘ç­‰ä»·ç‰©|cash.*reserve|æµåŠ¨æ€§é£é™©æ•å£).*?(\d+[,\d]*\.?\d*)\s*(äº¿|äº¿å…ƒ|ç™¾ä¸‡|million|billion|ä¸‡)',
    "credit_rating": r'(è¯„çº§|rating).*?(AAA|AA\+|AA|AA-|A\+|A|A-|BBB\+|BBB|BBB-|BB\+|BB|BB-)',
    "contingent_liability": r'(è¯‰è®¼|pending litigation|æˆ–æœ‰è´Ÿå€º).*?(\d+[,\d]*\.?\d*)\s*(äº¿|ä¸‡å…ƒ|USD|HKD)',
    "related_transaction": r'(å…³è”äº¤æ˜“é‡‘é¢|related party transaction).*?(\d+[,\d]*\.?\d*)\s*(äº¿|ä¸‡å…ƒ|HKD|USD)',
    "profit": r'(å‡€åˆ©æ¶¦|net profit|è¥æ”¶|revenue).*?(\d+[,\d]*\.?\d*)\s*(äº¿|äº¿å…ƒ|ä¸‡ç¾å…ƒ|USD)'
}

# ====================== åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ =======================
def init_kimi_client() -> OpenAI:
    """åˆå§‹åŒ–Kimiå¤§æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆå«é”™è¯¯å¤„ç†ï¼‰"""
    if not KIMI_API_KEY:
        raise ValueError("è¯·è®¾ç½®Kimi API Keyï¼ˆKIMI_API_KEYå˜é‡ï¼‰ï¼")
    try:
        client = OpenAI(api_key=KIMI_API_KEY, base_url=KIMI_BASE_URL)
        # æµ‹è¯•å®¢æˆ·ç«¯è¿é€šæ€§
        client.models.list()
        print(f"âœ… Kimiå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆæ¨¡å‹ï¼š{KIMI_MODEL}ï¼‰")
        return client
    except Exception as e:
        raise RuntimeError(f"Kimiå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}") from e

def init_chroma() -> chromadb.Collection:
    """åˆå§‹åŒ–Chromaå‘é‡åº“ï¼ˆå®‰å…¨åˆ›å»º/åˆ é™¤ï¼Œé¿å…æ–‡ä»¶å ç”¨ï¼‰"""
    # 1. å®‰å…¨åˆ é™¤æ—§å‘é‡åº“
    safe_delete_rag_db()
    # 2. ç¡®ä¿å‘é‡åº“ç›®å½•å­˜åœ¨
    ensure_dirs(RAG_DB_DIR)
    # 3. åˆ›å»ºChromaå®¢æˆ·ç«¯å’Œé›†åˆ
    emb_fn = ONNXMiniLM_L6_V2(preferred_providers=["CPUExecutionProvider"])
    chroma_client = chromadb.PersistentClient(path=RAG_DB_DIR)
    collection = chroma_client.create_collection(
        name="risk_entities",
        embedding_function=emb_fn,
        metadata={"description": "é‡‘èé£æ§å®ä½“å‘é‡åº“"}
    )
    print(f"âœ… Chromaå‘é‡åº“åˆå§‹åŒ–æˆåŠŸï¼ˆè·¯å¾„ï¼š{RAG_DB_DIR}ï¼‰")
    return collection

def safe_delete_rag_db() -> None:
    """å®‰å…¨åˆ é™¤æ—§å‘é‡åº“ï¼ˆå¤„ç†æ–‡ä»¶å ç”¨é—®é¢˜ï¼‰"""
    if not os.path.exists(RAG_DB_DIR):
        return
    print(f"ğŸ”„ æ£€æµ‹åˆ°æ—§å‘é‡åº“ï¼Œå°è¯•å®‰å…¨åˆ é™¤ï¼š{RAG_DB_DIR}")
    
    # 1. å…ˆå°è¯•Chromaå†…éƒ¨åˆ é™¤
    try:
        chroma_client = chromadb.PersistentClient(path=RAG_DB_DIR)
        chroma_client.delete_collection("risk_entities")
        print("âœ… Chromaé›†åˆå·²å†…éƒ¨åˆ é™¤")
    except Exception as e:
        print(f"âš ï¸ Chromaå†…éƒ¨åˆ é™¤å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰ï¼š{str(e)}")
    
    # 2. å¼ºåˆ¶åˆ é™¤ç›®å½•ï¼ˆé‡è¯•5æ¬¡ï¼Œå¤„ç†æ–‡ä»¶å ç”¨ï¼‰
    for retry in range(5):
        try:
            shutil.rmtree(RAG_DB_DIR)
            print(f"âœ… æ—§å‘é‡åº“ç›®å½•å·²åˆ é™¤ï¼ˆé‡è¯•{retry+1}æ¬¡ï¼‰")
            time.sleep(1)
            break
        except PermissionError:
            print(f"âš ï¸ æ–‡ä»¶è¢«å ç”¨ï¼Œ{2}ç§’åé‡è¯•ï¼ˆ{retry+1}/5ï¼‰")
            time.sleep(2)
        except Exception as e:
            print(f"âŒ åˆ é™¤å¤±è´¥ï¼ˆ{retry+1}/5ï¼‰ï¼š{str(e)}")
            time.sleep(1)

# ====================== å®ä½“æå–æ ¸å¿ƒé€»è¾‘ =======================
def load_risk_rules() -> Dict:
    """åŠ è½½é£é™©å®ä½“è§„åˆ™åº“ï¼ˆå®‰å…¨è¯»å–ï¼Œæ”¯æŒé»˜è®¤è§„åˆ™ï¼‰"""
    # 1. å°è¯•åŠ è½½è‡ªå®šä¹‰è§„åˆ™åº“
    rules = load_json_file(RULES_PATH)
    if rules and "entities" in rules:
        print(f"âœ… åŠ è½½è‡ªå®šä¹‰è§„åˆ™åº“æˆåŠŸï¼ˆå®ä½“ç±»å‹æ•°ï¼š{len(rules['entities'])}ï¼‰")
        return rules["entities"]
    
    # 2. æ— è‡ªå®šä¹‰è§„åˆ™æ—¶ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™ï¼ˆé¿å…è¿è¡Œå¤±è´¥ï¼‰
    print(f"âš ï¸ æœªæ‰¾åˆ°è‡ªå®šä¹‰è§„åˆ™åº“ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™ï¼ˆè·¯å¾„ï¼š{RULES_PATH}ï¼‰")
    default_rules = {
        "liquidity_risk": {
            "keywords": ["æµåŠ¨æ€§é£é™©", "ç°é‡‘å‚¨å¤‡ä¸è¶³", "æµåŠ¨æ€§æ•å£"],
            "risk_score": 25,
            "description": "æµåŠ¨æ€§é£é™©ï¼šå½±å“æœºæ„çŸ­æœŸå¿å€ºèƒ½åŠ›çš„é£é™©"
        },
        "credit_rating": {
            "keywords": ["ä¿¡ç”¨è¯„çº§ä¸‹è°ƒ", "AA+", "BBB-", "è¯„çº§å±•æœ›è´Ÿé¢"],
            "risk_score": 20,
            "description": "ä¿¡ç”¨è¯„çº§ï¼šåæ˜ ä¸»ä½“ä¿¡ç”¨é£é™©çš„è¯„çº§ç»“æœ"
        },
        "related_transaction": {
            "keywords": ["å…³è”äº¤æ˜“", "å…³è”æ–¹èµ„é‡‘å ç”¨", "éå…¬å…å…³è”äº¤æ˜“"],
            "risk_score": 15,
            "description": "å…³è”äº¤æ˜“ï¼šå¯èƒ½å­˜åœ¨åˆ©ç›Šè¾“é€çš„äº¤æ˜“è¡Œä¸º"
        },
        "law_risk": {
            "keywords": ["è¯‰è®¼", "è¡Œæ”¿å¤„ç½š", "åˆè§„é£é™©", "è¿åä¸Šå¸‚è§„åˆ™"],
            "risk_score": 30,
            "description": "æ³•å¾‹åˆè§„é£é™©ï¼šæ¶‰åŠè¯‰è®¼ã€å¤„ç½šçš„é£é™©"
        }
    }
    return default_rules

def extract_rule_based_entities(text: str, rules: Dict) -> List[Dict]:
    """åŸºäºè§„åˆ™æå–å®ä½“ï¼ˆå«é‡‘èæ•°å­—è¯†åˆ«ï¼Œå»é‡+æ¸…æ´—ï¼‰"""
    entities = []
    seen = set()  # å»é‡æ ‡è®°ï¼ˆé¿å…é‡å¤æå–åŒä¸€å®ä½“ï¼‰
    
    # 1. æ¸…æ´—æ–‡æœ¬ï¼ˆç»Ÿä¸€æ ‡ç‚¹ã€å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
    text = clean_text(text)
    if not text:
        return entities
    
    # 2. å…³é”®è¯åŒ¹é…æå–ï¼ˆéæ•°å­—å®ä½“ï¼‰
    print(f"ğŸ” å¼€å§‹è§„åˆ™æå–ï¼ˆæ–‡æœ¬é•¿åº¦ï¼š{len(text)}å­—ç¬¦ï¼‰")
    for ent_type, config in rules.items():
        for keyword in config["keywords"]:
            # æ­£åˆ™åŒ¹é…å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼Œç²¾å‡†åŒ¹é…å•è¯è¾¹ç•Œï¼‰
            pattern = rf'\b{re.escape(keyword)}\b'
            for match in re.finditer(pattern, text, re.IGNORECASE):
                start = match.start()
                end = match.end()
                # å»é‡é”®ï¼ˆç±»å‹+å…³é”®è¯+ä½ç½®ï¼Œé¿å…åŒä¸€å®ä½“é‡å¤æ·»åŠ ï¼‰
                dedup_key = f"{ent_type}_{keyword}_{start}"
                if dedup_key in seen:
                    continue
                seen.add(dedup_key)
                
                # æå–ä¸Šä¸‹æ–‡ï¼ˆå‰åå„80å­—ç¬¦ï¼Œä¾¿äºåç»­ç†è§£å®ä½“åœºæ™¯ï¼‰
                context = text[max(0, start-80):end+80].replace("\n", " ").strip()
                
                entities.append({
                    "type": ent_type,
                    "text": keyword,
                    "start": start,
                    "end": end,
                    "context": context,
                    "confidence": 0.92,  # è§„åˆ™æå–ç½®ä¿¡åº¦ï¼ˆå›ºå®šé«˜å€¼ï¼‰
                    "risk_score": config["risk_score"],
                    "description": config["description"]
                })
    
    # 3. æ•°å­—å®ä½“æå–ï¼ˆå¦‚é‡‘é¢ã€è¯„çº§ï¼‰
    for ent_type, pattern in NUM_PATTERNS.items():
        for match in re.finditer(pattern, text, re.IGNORECASE | re.DOTALL):
            # è§£æåŒ¹é…ç»“æœï¼ˆç¡®ä¿åˆ†ç»„å­˜åœ¨ï¼‰
            if len(match.groups()) < 2:
                continue
            metric_name = match.group(1).strip()  # æŒ‡æ ‡åç§°ï¼ˆå¦‚â€œæµåŠ¨æ€§é£é™©æ•å£â€ï¼‰
            amount = match.group(2).replace(",", "")  # é‡‘é¢ï¼ˆå»é™¤åƒåˆ†ä½é€—å·ï¼‰
            unit = match.group(3) if len(match.groups()) > 2 else ""  # å•ä½ï¼ˆå¦‚â€œäº¿å…ƒâ€ï¼‰
            ent_text = f"{metric_name}{amount}{unit}"  # å®Œæ•´å®ä½“æ–‡æœ¬
            start = match.start()
            end = match.end()
            
            # å»é‡
            dedup_key = f"{ent_type}_num_{start}"
            if dedup_key in seen:
                continue
            seen.add(dedup_key)
            
            entities.append({
                "type": ent_type,
                "text": ent_text,
                "start": start,
                "end": end,
                "context": match.group(0).replace("\n", " ").strip(),
                "confidence": 0.96,  # æ•°å­—æå–ç½®ä¿¡åº¦ï¼ˆæ›´é«˜ï¼‰
                "risk_score": rules.get(ent_type, {}).get("risk_score", 15),  # é»˜è®¤é£é™©åˆ†
                "description": rules.get(ent_type, {}).get("description", f"é‡‘èæ•°å­—æŒ‡æ ‡ï¼š{metric_name}")
            })
    
    print(f"âœ… è§„åˆ™æå–å®Œæˆï¼ˆå®ä½“æ•°ï¼š{len(entities)}ï¼‰")
    return entities

def extract_bert_entities(text: str) -> List[Dict]:
    """åŸºäºè®­ç»ƒå¥½çš„BERTæ¨¡å‹æå–å®ä½“ï¼ˆå¯é€‰å¢å¼ºï¼Œå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼‰"""
    try:
        from transformers import (
            AutoModelForTokenClassification, 
            AutoTokenizer, 
            pipeline
        )
        # 1. æ£€æŸ¥BERTæ¨¡å‹è·¯å¾„
        if not os.path.exists(BERT_MODEL_PATH):
            raise FileNotFoundError(f"BERTæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼š{BERT_MODEL_PATH}")
        
        # 2. åŠ è½½BERTæ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆé€‚é…ä½ çš„è®­ç»ƒç»“æœï¼‰
        tokenizer = AutoTokenizer.from_pretrained(
            BERT_MODEL_PATH,
            local_files_only=True,
            model_max_length=512
        )
        model = AutoModelForTokenClassification.from_pretrained(
            BERT_MODEL_PATH,
            local_files_only=True,
            id2label={0:"O",1:"B-DATE",2:"I-DATE",3:"B-PER",4:"I-PER",
                      5:"B-ORG",6:"I-ORG",7:"B-MONEY",8:"I-MONEY",
                      9:"B-RISK",10:"I-RISK",11:"B-SEC",12:"I-SEC",
                      13:"B-REG",14:"I-REG",15:"B-LAW",16:"I-LAW"}  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
        )
        
        # 3. å¤„ç†é•¿æ–‡æœ¬ï¼ˆåˆ†ç‰‡é¿å…å†…å­˜æº¢å‡ºï¼‰
        bert_entities = []
        chunk_size = 450  # æ–‡æœ¬åˆ†ç‰‡å¤§å°ï¼ˆé€‚é…512tokenï¼‰
        overlap = 50
        total_chunks = max(1, (len(text) + chunk_size - overlap - 1) // (chunk_size - overlap))
        
        print(f"ğŸ” å¼€å§‹BERTå®ä½“æå–ï¼ˆåˆ†ç‰‡æ•°ï¼š{total_chunks}ï¼‰")
        for i in tqdm(range(0, len(text), chunk_size - overlap), desc="BERTæå–è¿›åº¦"):
            chunk = text[i:i+chunk_size]
            # 4. BERTå®ä½“æå–ï¼ˆè¿‡æ»¤ä½ç½®ä¿¡åº¦ï¼‰
            ner_pipe = pipeline(
                "ner",
                model=model,
                tokenizer=tokenizer,
                aggregation_strategy="simple",
                device=-1  # å¼ºåˆ¶CPUï¼ˆé¿å…GPUå†…å­˜ä¸è¶³ï¼‰
            )
            results = ner_pipe(chunk)
            
            # 5. å¤„ç†BERTç»“æœï¼ˆè¡¥å……åˆ†ç‰‡ä½ç½®åç§»ï¼‰
            for res in results:
                if res["score"] < 0.8:  # è¿‡æ»¤ä½ç½®ä¿¡åº¦å®ä½“
                    continue
                # ä¿®æ­£å®ä½“åœ¨å…¨æ–‡ä¸­çš„ä½ç½®
                res["start"] += i
                res["end"] += i
                bert_entities.append({
                    "type": res["entity_group"],
                    "text": res["word"],
                    "start": res["start"],
                    "end": res["end"],
                    "context": text[max(0, res["start"]-60):res["end"]+60].replace("\n", " "),
                    "confidence": round(res["score"], 3),
                    "risk_score": 10,  # BERTå®ä½“é»˜è®¤é£é™©åˆ†
                    "description": f"BERTè‡ªåŠ¨è¯†åˆ«ï¼š{res['entity_group']}ç±»å‹å®ä½“"
                })
        
        print(f"âœ… BERTæå–å®Œæˆï¼ˆå®ä½“æ•°ï¼š{len(bert_entities)}ï¼‰")
        return bert_entities
    except Exception as e:
        print(f"âš ï¸ BERTå®ä½“æå–è·³è¿‡ï¼ˆåŸå› ï¼š{str(e)}ï¼‰")
        return []

def merge_entities(rule_ents: List[Dict], bert_ents: List[Dict]) -> List[Dict]:
    """åˆå¹¶è§„åˆ™æå–å’ŒBERTæå–çš„å®ä½“ï¼ˆå»é‡ï¼Œä¿ç•™é«˜ç½®ä¿¡åº¦ï¼‰"""
    merged = {}
    all_ents = rule_ents + bert_ents
    
    for ent in all_ents:
        # å»é‡é”®ï¼šå®ä½“ç±»å‹+æ–‡æœ¬+èµ·å§‹ä½ç½®ï¼ˆé¿å…åŒä¸€å®ä½“é‡å¤ï¼‰
        dedup_key = f"{ent['type']}_{ent['text']}_{ent['start']}"
        # ä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„å®ä½“
        if dedup_key not in merged or ent["confidence"] > merged[dedup_key]["confidence"]:
            merged[dedup_key] = ent
    
    final_ents = list(merged.values())
    print(f"âœ… å®ä½“åˆå¹¶å»é‡å®Œæˆï¼ˆæœ€ç»ˆå®ä½“æ•°ï¼š{len(final_ents)}ï¼‰")
    return final_ents

# ====================== RAGå‘é‡åº“ä¸é—®ç­” =======================
def build_rag_db(entities: List[Dict], collection: chromadb.Collection) -> None:
    """åŸºäºæå–çš„å®ä½“æ„å»ºChromaå‘é‡åº“"""
    if not entities:
        print("âš ï¸ æ— å®ä½“å¯æ„å»ºå‘é‡åº“ï¼Œè·³è¿‡")
        return
    
    # æ„å»ºå‘é‡åº“æ‰€éœ€æ•°æ®ï¼ˆæ–‡æ¡£=å®ä½“æè¿°+ä¸Šä¸‹æ–‡ï¼Œå…ƒæ•°æ®=å®ä½“å±æ€§ï¼‰
    docs = [
        f"ã€{ent['type']}ã€‘{ent['description']}\nå®ä½“å†…å®¹ï¼š{ent['text']}\nä¸Šä¸‹æ–‡ï¼š{ent['context']}"
        for ent in entities
    ]
    metadatas = [
        {
            "type": ent["type"],
            "text": ent["text"],
            "risk_score": ent["risk_score"],
            "confidence": ent["confidence"],
            "start": ent["start"],
            "end": ent["end"]
        } for ent in entities
    ]
    ids = [f"ent_{i}" for i in range(len(entities))]
    
    # æ‰¹é‡æ·»åŠ åˆ°Chromaï¼ˆå¤„ç†å¯èƒ½çš„å¼‚å¸¸ï¼‰
    try:
        collection.add(documents=docs, metadatas=metadatas, ids=ids)
        print(f"âœ… RAGå‘é‡åº“æ„å»ºå®Œæˆï¼ˆå®ä½“æ•°ï¼š{len(entities)}ï¼Œå‘é‡æ•°ï¼š{len(docs)}ï¼‰")
    except Exception as e:
        raise RuntimeError(f"å‘é‡åº“æ„å»ºå¤±è´¥ï¼š{str(e)}") from e

def rag_qa(question: str, collection: chromadb.Collection, kimi_client: OpenAI) -> str:
    """RAGé—®ç­”ï¼šæ£€ç´¢å‘é‡åº“+Kimiç”Ÿæˆå›ç­”"""
    # 1. æ£€ç´¢ç›¸å…³å®ä½“ï¼ˆTop4ï¼‰
    print(f"\nğŸ” æ£€ç´¢ç›¸å…³å®ä½“ï¼ˆé—®é¢˜ï¼š{question[:50]}...ï¼‰")
    try:
        query_res = collection.query(query_texts=[question], n_results=4)
        docs = query_res.get("documents", [[]])[0]
        metadatas = query_res.get("metadatas", [[]])[0]
        if not docs:
            return "æœªæ£€ç´¢åˆ°ä¸é—®é¢˜ç›¸å…³çš„å®ä½“ä¿¡æ¯ã€‚"
    except Exception as e:
        return f"å‘é‡åº“æ£€ç´¢å¤±è´¥ï¼š{str(e)}"
    
    # 2. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼‰
    context = "\n\n".join([
        f"ã€{meta['type']}ã€‘{meta['text']}\né£é™©åˆ†ï¼š{meta['risk_score']}\nä¸Šä¸‹æ–‡ï¼š{doc.split('ä¸Šä¸‹æ–‡ï¼š')[-1].strip()}"
        for doc, meta in zip(docs, metadatas)
    ])
    
    # 3. è°ƒç”¨Kimiç”Ÿæˆå›ç­”ï¼ˆé‡‘èé£æ§åœºæ™¯promptï¼‰
    prompt = f"""
ä½ æ˜¯ä¸“ä¸šé‡‘èé£æ§é¡¾é—®ï¼ŒåŸºäºä»¥ä¸‹å®ä½“ä¿¡æ¯å›ç­”é—®é¢˜ï¼Œè¦æ±‚ï¼š
1. ä¸¥æ ¼å¼•ç”¨ä¸Šä¸‹æ–‡å®ä½“ï¼Œä¸ç¼–é€ ä¿¡æ¯ï¼›
2. å›ç­”ç®€æ´ï¼ˆ<100å­—ï¼‰ï¼Œé‡ç‚¹çªå‡ºé£é™©ç‚¹/å…³é”®æŒ‡æ ‡ï¼›
3. åŒ…å«å®ä½“ç±»å‹å’Œé£é™©è¯„åˆ†ï¼ˆå¦‚æœ‰ï¼‰ã€‚

ä¸Šä¸‹æ–‡å®ä½“ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}
"""
    try:
        response = kimi_client.chat.completions.create(
            model=KIMI_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,  # ä½æ¸©åº¦ä¿è¯å›ç­”ç¨³å®š
            max_tokens=512
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Kimiå›ç­”ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"

# ====================== ä¸»æµç¨‹ =======================
def main():
    try:
        # 1. åˆå§‹åŒ–ç»„ä»¶ï¼ˆKimi+Chromaï¼‰
        print("="*50)
        print("ğŸš€ å¼€å§‹é‡‘èé£æ§å®ä½“æå–ä¸RAGé—®ç­”æµç¨‹")
        print("="*50)
        kimi_client = init_kimi_client()
        chroma_collection = init_chroma()
        
        # 2. åŠ è½½è§„åˆ™åº“
        print("\n" + "="*30)
        risk_rules = load_risk_rules()
        
        # 3. è¯»å–å¾…å¤„ç†æ–‡æœ¬ï¼ˆdocsç›®å½•ä¸‹æ‰€æœ‰txtï¼‰
        print("\n" + "="*30)
        txt_files = [f for f in os.listdir(DOCS_DIR) if f.lower().endswith(".txt")]
        if not txt_files:
            raise FileNotFoundError(f"æœªåœ¨{DOCS_DIR}ç›®å½•æ‰¾åˆ°txtæ–‡ä»¶ï¼Œè¯·æ”¾å…¥å¾…å¤„ç†æ–‡æ¡£ï¼")
        # è¯»å–ç¬¬ä¸€ä¸ªtxtæ–‡ä»¶ï¼ˆå¦‚éœ€å¤„ç†å¤šæ–‡ä»¶ï¼Œå¯å¾ªç¯éå†ï¼‰
        target_file = txt_files[0]
        file_path = normalize_path(os.path.join(DOCS_DIR, target_file))
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
        print(f"âœ… è¯»å–æ–‡æ¡£æˆåŠŸï¼ˆæ–‡ä»¶åï¼š{target_file}ï¼Œå­—ç¬¦æ•°ï¼š{len(text)}ï¼‰")
        
        # 4. å¤šæ¨¡å¼å®ä½“æå–ï¼ˆè§„åˆ™+BERTï¼‰
        print("\n" + "="*30)
        rule_entities = extract_rule_based_entities(text, risk_rules)
        bert_entities = extract_bert_entities(text)
        final_entities = merge_entities(rule_entities, bert_entities)
        
        # 5. è®¡ç®—é£é™©ç­‰çº§
        total_risk = sum(ent["risk_score"] for ent in final_entities)
        risk_level = calculate_risk_level(total_risk)
        print(f"ğŸ“Š é£é™©è¯„ä¼°ç»“æœï¼šæ€»é£é™©åˆ†={total_risk} | é£é™©ç­‰çº§={risk_level}")
        
        # 6. ä¿å­˜å®ä½“ç»“æœåˆ°JSON
        print("\n" + "="*30)
        result_data = {
            "extracted_at": datetime.now().isoformat(),
            "source_file": target_file,
            "total_entities": len(final_entities),
            "total_risk_score": total_risk,
            "risk_level": risk_level,
            "entities": final_entities
        }
        if save_json_file(result_data, OUTPUT_JSON):
            print(f"âœ… å®ä½“ç»“æœä¿å­˜æˆåŠŸï¼ˆè·¯å¾„ï¼š{OUTPUT_JSON}ï¼‰")
        else:
            print(f"âš ï¸ å®ä½“ç»“æœä¿å­˜å¤±è´¥ï¼ˆè·¯å¾„ï¼š{OUTPUT_JSON}ï¼‰")
        
        # 7. æ„å»ºRAGå‘é‡åº“
        print("\n" + "="*30)
        build_rag_db(final_entities, chroma_collection)
        
        # 8. å¾ªç¯é—®ç­”äº¤äº’
        print("\n" + "="*50)
        print("ğŸ’¬ RAGé£æ§é—®ç­”ç³»ç»Ÿå·²å°±ç»ªï¼ˆè¾“å…¥'exit'é€€å‡ºï¼‰")
        print("="*50)
        while True:
            question = input("\nè¯·è¾“å…¥é—®é¢˜ï¼š").strip()
            if question.lower() in ["exit", "quit", "é€€å‡º"]:
                print("ğŸ‘‹ å†è§ï¼")
                break
            if not question:
                continue
            answer = rag_qa(question, chroma_collection, kimi_client)
            print(f"\nğŸ“ å›ç­”ï¼š{answer}")
    
    except Exception as e:
        print(f"\nâŒ æµç¨‹è¿è¡Œå¤±è´¥ï¼š{str(e)}")
        exit(1)

if __name__ == "__main__":
    main()