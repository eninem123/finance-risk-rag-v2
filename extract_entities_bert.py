import os
import json
import torch
import numpy as np  # ç¡®ä¿å¯¼å…¥numpy
from transformers import AutoTokenizer, AutoModelForTokenClassification
from collections import defaultdict
from tqdm import tqdm

# ====================== é…ç½®å‚æ•° =======================
MODEL_PATH = "bert_ner_model/best_model"
DOCS_DIR = "docs"
OUTPUT_FILE = os.path.join(DOCS_DIR, "entities_extracted.json")
LABEL_LIST = [
    "O", "B-DATE", "I-DATE", "B-PER", "I-PER", 
    "B-ORG", "I-ORG", "B-MONEY", "I-MONEY", 
    "B-RISK", "I-RISK", "B-SEC", "I-SEC", 
    "B-REG", "I-REG", "B-LAW", "I-LAW"
]
id2label = {i: label for i, label in enumerate(LABEL_LIST)}
label2id = {v: k for k, v in id2label.items()}
MAX_SEQ_LEN = 512
CHUNK_SIZE = 450
OVERLAP = 50


# ====================== åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ =======================
def load_model_tokenizer():
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            local_files_only=True,
            model_max_length=MAX_SEQ_LEN
        )
        model = AutoModelForTokenClassification.from_pretrained(
            MODEL_PATH,
            local_files_only=True,
            id2label=id2label,
            label2id=label2id
        )
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model.eval()
        print(f"ä½¿ç”¨è®¾å¤‡ï¼š{device}")
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼š{MODEL_PATH}\n")
        return model, tokenizer, device
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return None, None, None


# ====================== å®ä½“èšåˆé€»è¾‘ =======================
def aggregate_entities(tokens, predictions, scores, tokenizer):
    entities = []
    current_entity = None
    for token, pred, score in zip(tokens, predictions, scores):
        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:
            continue
        if pred.startswith("B-"):
            if current_entity:
                entities.append(current_entity)
            entity_type = pred[2:]
            current_entity = {
                "word": token.replace("##", ""),
                "entity_group": entity_type,
                "start": None,
                "end": None,
                "score": score
            }
        elif pred.startswith("I-") and current_entity:
            entity_type = pred[2:]
            if entity_type == current_entity["entity_group"]:
                current_entity["word"] += token.replace("##", "")
                current_entity["score"] = (current_entity["score"] + score) / 2
        else:
            if current_entity:
                entities.append(current_entity)
                current_entity = None
    if current_entity:
        entities.append(current_entity)
    return entities


# ====================== å®ä½“æŠ½å–é€»è¾‘ï¼ˆä¿®å¤æ•°æ®ç±»å‹ï¼‰=======================
def extract_entities_from_text(text: str, model, tokenizer, device, filename):
    if not model or not tokenizer or not text:
        return []
    
    total_chunks = max(1, (len(text) + CHUNK_SIZE - OVERLAP - 1) // (CHUNK_SIZE - OVERLAP))
    all_entities = []
    
    for i in tqdm(range(0, len(text), CHUNK_SIZE - OVERLAP), 
                  desc=f"å¤„ç† {filename} åˆ†ç‰‡", 
                  total=total_chunks, 
                  unit="ç‰‡"):
        end = i + CHUNK_SIZE
        chunk = text[i:end]
        
        encoding = tokenizer(
            chunk,
            return_tensors="pt",
            max_length=MAX_SEQ_LEN,
            padding="max_length",
            truncation=True,
            return_offsets_mapping=True
        )
        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        offset_mapping = encoding["offset_mapping"].squeeze(0).numpy()
        
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=2).squeeze(0).cpu().numpy()
        scores = torch.max(torch.softmax(logits, dim=2), dim=2).values.squeeze(0).cpu().numpy()
        
        pred_labels = [id2label[pred] for pred in predictions]
        tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0).cpu().numpy())
        chunk_entities = aggregate_entities(tokens, pred_labels, scores, tokenizer)
        
        # å…³é”®ä¿®æ­£ï¼šå°†numpy.int64è½¬ä¸ºPythonåŸç”Ÿint
        for ent in chunk_entities:
            ent_start = None
            ent_end = None
            for idx, (start, end) in enumerate(offset_mapping):
                if start == 0 and end == 0:
                    continue
                token = tokens[idx].replace("##", "")
                if ent["word"].startswith(token) and ent_start is None:
                    ent_start = int(start + i)  # è½¬ä¸ºPython int
                if ent["word"].endswith(token) and ent_end is None:
                    ent_end = int(end + i)    # è½¬ä¸ºPython int
            if ent_start is not None and ent_end is not None:
                ent["start"] = ent_start
                ent["end"] = ent_end
                all_entities.append(ent)
    
    # å»é‡
    # å»é‡åæ·»åŠ è¿‡æ»¤é€»è¾‘
    unique_entities = []
    seen = set()
    for ent in all_entities:
        if "start" not in ent or "end" not in ent:
            continue
        # è¿‡æ»¤è§„åˆ™ï¼š
        # 1. è¿‡æ»¤å•ä¸ªå­—çš„éé‡‘é¢/éæ—¥æœŸå®ä½“ï¼ˆORG/REG/LAWç­‰å•ä¸ªå­—å‡ ä¹éƒ½æ˜¯è¯¯åˆ¤ï¼‰
        entity_type = ent["entity_group"]
        entity_word = ent["word"]
        if len(entity_word) == 1 and entity_type not in ["MONEY", "DATE"]:
            continue
        # 2. è¿‡æ»¤ç½®ä¿¡åº¦ä½äº0.5çš„å®ä½“ï¼ˆä½ç½®ä¿¡åº¦å¤§æ¦‚ç‡æ˜¯è¯¯åˆ¤ï¼‰
        if ent["score"] < 0.5:
            continue
        # 3. å»é‡
        key = (entity_word, ent["start"], ent["end"], entity_type)
        if key not in seen:
            seen.add(key)
            unique_entities.append(ent)
    
    return unique_entities


# ====================== ä¸»å‡½æ•° =======================
def main():
    model, tokenizer, device = load_model_tokenizer()
    if not model or not tokenizer:
        return
    
    if not os.path.exists(DOCS_DIR):
        os.makedirs(DOCS_DIR)
        print(f"âš ï¸ å·²åˆ›å»ºæ–‡æ¡£ç›®å½•ï¼š{DOCS_DIR}ï¼Œè¯·æ”¾å…¥txtæ–‡ä»¶åé‡è¯•")
        return
    
    txt_files = [f for f in os.listdir(DOCS_DIR) if f.lower().endswith(".txt")]
    if not txt_files:
        print(f"âš ï¸ æœªåœ¨{DOCS_DIR}æ‰¾åˆ°txtæ–‡ä»¶")
        return
    
    results = {}
    for filename in tqdm(txt_files, desc="æ€»ä½“è¿›åº¦", total=len(txt_files), unit="æ–‡ä»¶"):
        file_path = os.path.join(DOCS_DIR, filename)
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read().strip()
            print(f"\nå¼€å§‹å¤„ç†ï¼š{filename}ï¼ˆé•¿åº¦ï¼š{len(text)}å­—ç¬¦ï¼‰")
            
            entities = extract_entities_from_text(text, model, tokenizer, device, filename)
            formatted_entities = [
                {
                    "å®ä½“å†…å®¹": ent["word"],
                    "å®ä½“ç±»å‹": ent["entity_group"],
                    "èµ·å§‹ä½ç½®": ent["start"],
                    "ç»“æŸä½ç½®": ent["end"],
                    "ç½®ä¿¡åº¦": round(ent["score"], 4)
                } for ent in entities
            ]
            results[filename] = formatted_entities
            print(f"âœ… {filename} å¤„ç†å®Œæˆï¼ŒæŠ½å–åˆ° {len(formatted_entities)} ä¸ªå®ä½“")
        except Exception as e:
            print(f"âŒ {filename} å¤„ç†å¤±è´¥ï¼š{str(e)}")
    
    # ä¿å­˜ç»“æœï¼ˆç°åœ¨æ‰€æœ‰æ•°æ®éƒ½æ˜¯PythonåŸç”Ÿç±»å‹ï¼Œå¯æ­£å¸¸åºåˆ—åŒ–ï¼‰
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜è‡³ï¼š{OUTPUT_FILE}")


if __name__ == "__main__":
    main()